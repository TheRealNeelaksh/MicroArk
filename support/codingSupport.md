# ðŸ§­ PHASE 0 â€” Project Setup (30â€“60 min)

## ðŸŽ¯ Goal

Create working environment.

## âœ… Do This

1. Create new folder:

```
wildlife-ai/
```

2. Create virtual environment (optional but good):

```
python -m venv venv
```

3. Install libraries:

```
pip install pandas numpy scikit-learn matplotlib seaborn streamlit joblib
```

4. Create structure:

```
wildlife-ai/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ model/
â”œâ”€â”€ app.py
â””â”€â”€ train.py
```

---

# ðŸ§­ PHASE 1 â€” Get Data from Metalog (3â€“4 hrs)

## ðŸŽ¯ Goal

Download metadata CSV.

### âœ… Steps

1. Go to:
   Metalog â†’ Animal samples â†’ Explore all samples

2. Export metadata as CSV.

3. Save as:

```
data/metalog_raw.csv
```

---

# ðŸ§­ PHASE 2 â€” Data Cleaning & Feature Creation (3â€“5 hrs)

## ðŸŽ¯ Goal

Turn sample-level data into species-level features.

---

## ðŸ§© Step 2.1 â€” Load Data

In `train.py`:

```python
import pandas as pd

df = pd.read_csv("data/metalog_raw.csv")
print(df.head())
```

---

## ðŸ§© Step 2.2 â€” Clean

Remove:

```python
df = df.dropna(subset=["host_species"])
```

Remove species with < 5 samples:

```python
species_counts = df["host_species"].value_counts()
valid_species = species_counts[species_counts >= 5].index
df = df[df["host_species"].isin(valid_species)]
```

---

## ðŸ§© Step 2.3 â€” Create Features Per Species

Group by species:

```python
grouped = df.groupby("host_species")

features = pd.DataFrame({
    "num_samples": grouped.size(),
    "num_countries": grouped["country"].nunique(),
    "year_span": grouped["collection_year"].max() - grouped["collection_year"].min(),
    "lat_variance": grouped["latitude"].var(),
    "long_variance": grouped["longitude"].var()
})

features = features.fillna(0)
features.reset_index(inplace=True)
```

Save:

```python
features.to_csv("data/species_features.csv", index=False)
```

---

# ðŸ§­ PHASE 3 â€” Add IUCN Labels (2â€“3 hrs)

## ðŸŽ¯ Goal

Add extinction risk class.

Manually create CSV:

```
data/iucn_labels.csv
```

Format:

| host_species    | risk_label |
| --------------- | ---------- |
| Panthera tigris | 4          |
| Bos taurus      | 0          |

Mapping:
LC=0, NT=1, VU=2, EN=3, CR=4

---

## Merge:

```python
labels = pd.read_csv("data/iucn_labels.csv")
final_df = features.merge(labels, on="host_species")
```

Save:

```python
final_df.to_csv("data/final_dataset.csv", index=False)
```

---

# ðŸ§­ PHASE 4 â€” Train ML Model (3â€“4 hrs)

## ðŸŽ¯ Goal

Train Random Forest.

---

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import joblib

X = final_df.drop(["host_species", "risk_label"], axis=1)
y = final_df["risk_label"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

model = RandomForestClassifier()
model.fit(X_train, y_train)

preds = model.predict(X_test)
print(classification_report(y_test, preds))

joblib.dump(model, "model/extinction_model.pkl")
```

---

# ðŸ§­ PHASE 5 â€” Add Early Warning Logic (1â€“2 hrs)

Add simple rule:

```python
def early_warning(row):
    if row["year_span"] < 5 and row["lat_variance"] < 1:
        return 1
    return 0

final_df["early_warning_flag"] = final_df.apply(early_warning, axis=1)
```

---

# ðŸ§­ PHASE 6 â€” Explainability (2 hrs)

```python
import matplotlib.pyplot as plt

importances = model.feature_importances_
plt.bar(X.columns, importances)
plt.xticks(rotation=45)
plt.show()
```

Save figure for presentation.

---

# ðŸ§­ PHASE 7 â€” Build Streamlit App (3â€“4 hrs)

In `app.py`:

```python
import streamlit as st
import pandas as pd
import joblib

model = joblib.load("model/extinction_model.pkl")
data = pd.read_csv("data/final_dataset.csv")

st.title("Wildlife Extinction Risk Predictor")

species = st.selectbox("Select Species", data["host_species"])

row = data[data["host_species"] == species]
X = row.drop(["host_species", "risk_label"], axis=1)

prediction = model.predict(X)[0]
prob = model.predict_proba(X)[0].max()

st.write("Predicted Risk Level:", prediction)
st.write("Confidence:", prob)
```

Run:

```
streamlit run app.py
```

---

# ðŸ§­ PHASE 8 â€” Testing & Polishing (2â€“3 hrs)

* Test 5 species
* Check prediction
* Screenshot app
* Prepare demo narrative

---

# ðŸ§  Total Time Estimate

| Phase          | Time    |
| -------------- | ------- |
| Setup          | 1 hr    |
| Data cleaning  | 4 hr    |
| Labels         | 3 hr    |
| Model          | 4 hr    |
| Explainability | 2 hr    |
| App            | 4 hr    |
| Polish         | 2 hr    |
| **Total**      | ~20 hrs |

---

# ðŸŽ¯ Final Output

You now have:

* AI model
* Early warning rule
* Web app
* Real dataset
* Conservation narrative
